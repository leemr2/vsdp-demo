# Emerging Technologies for Eye-Driven Neural Interfaces (2026–2029)

Non-invasive **eye-driven neural interfaces** are poised to augment XR/VR/AR control by interpreting signals from the visual system – including eye movements, pupil dynamics, and visually-evoked brain responses – as intuitive inputs. Below we outline key emerging technologies expected to mature by 2029, highlighting their development stage, leading players, and relevance to XR interfaces.

## Smart Contact Lens Interfaces

**Smart AR contact lenses** are progressing from lab prototypes toward practical use in the late 2020s. These lenses embed ultra-small displays and sensors directly on the eye, offering hands-free AR overlays and potentially gathering ocular biosignals. For example, Mojo Vision demonstrated a contact lens with a 14,000 ppi micro-LED display and wireless power in 2022[[1\]](https://www.uctoday.com/immersive-workplace-xr-tech/augmented-reality-contact-lenses-will-they-ever-be-a-reality/#:~:text=company that was previously leading,connected to an external battery)[[2\]](https://www.uctoday.com/immersive-workplace-xr-tech/augmented-reality-contact-lenses-will-they-ever-be-a-reality/#:~:text=alongside software application prototyping%2C were,still necessary). While Mojo paused its lens project due to funding, other ventures have stepped in. **XPANCEO** (Dubai) has unveiled multiple smart lens prototypes and plans initial tests of a fully integrated AR contact lens by 2026[[3\]](https://www.xpanceo.com/newsroom/gitex-press-release#:~:text=Initial testing of a fully,integrated into a single device). At GITEX 2024, XPANCEO showed lenses for 3D AR vision and even a “data reading” lens that wirelessly transmits data and sensor readings, allowing real-time control and biometric feedback through the lens[[4\]](https://www.xpanceo.com/newsroom/gitex-press-release#:~:text=Another innovation presented was the,jpg). Similarly, Belgium-based **Azalea Vision** is developing the **ALMA** smart lens to aid low vision, blending augmented reality with vision correction[[5\]](https://www.uctoday.com/immersive-workplace-xr-tech/augmented-reality-contact-lenses-will-they-ever-be-a-reality/#:~:text=Innovators believe high,to treat visual health issues). These efforts indicate that by 2029, smart contact lenses with built-in displays and eye sensors could serve as an **ultimate wearable interface** – enabling XR interactions (like navigating menus or zooming in) through natural eye focus or blink patterns, without any bulky headgear.



## Skin-Like Wearable Eye Sensors

Next-generation **wearable biosensors** are emerging in the form of ultrathin, skin-conformal “electronic tattoos” and flexible electrodes that can unobtrusively capture eye-related neural signals. Researchers at UT Austin, for instance, have developed a wireless **forehead e-tattoo** that records EEG (brainwaves) and EOG (eye movement signals) to gauge cognitive workload[[6\]](https://spectrum.ieee.org/electronic-tattoo#:~:text=Researchers at the University of,movements to gauge mental strain)[[7\]](https://www.ae.utexas.edu/news/stressed-or-bored-at-work-new-electronic-tattoo-can-help#:~:text=The e,for comfort and clear signals). This patch-like device adheres to the face like a temporary tattoo and features stretchable, carbon-based electrodes arranged in wavy patterns, allowing it to conform to facial skin (including around the eyes) for stable, high-fidelity signals[[7\]](https://www.ae.utexas.edu/news/stressed-or-bored-at-work-new-electronic-tattoo-can-help#:~:text=The e,for comfort and clear signals)[[8\]](https://www.ae.utexas.edu/news/stressed-or-bored-at-work-new-electronic-tattoo-can-help#:~:text=from the brain and eye,for comfort and clear signals). Importantly, it operates without the gels or wires of traditional EEG caps, using a tiny onboard battery and Bluetooth. In tests, the e-tattoo accurately detected changes in mental strain and even predicted fatigue levels via eye and brain signals, using a trained AI model[[9\]](https://www.ae.utexas.edu/news/stressed-or-bored-at-work-new-electronic-tattoo-can-help#:~:text=The device didn’t stop at,can potentially predict mental fatigue)[[10\]](https://www.ae.utexas.edu/news/stressed-or-bored-at-work-new-electronic-tattoo-can-help#:~:text=researchers trained a computer model,can potentially predict mental fatigue). By 2029, such **skin-like electrode arrays** are expected to transition from lab demos to real-world applications. We may see **adhesive eye-region sensors** (e.g. around the temples or on eyelids) integrated into AR/VR masks or helmets, measuring blinks, electroretinography (ERG) signals, and ocular muscle activity. These will enable subtle eye gestures (winks, prolonged blinks, gaze dwell) as control inputs. Companies are also pushing graphene-based electronic tattoos for biosensing – offering “invisible” yet high-quality electrodes for continuous monitoring[[6\]](https://spectrum.ieee.org/electronic-tattoo#:~:text=Researchers at the University of,movements to gauge mental strain)[[11\]](https://spectrum.ieee.org/electronic-tattoo#:~:text=circuit toward the temples%2C cheeks%2C,and to stabilize the signal). Overall, **conformal bio-sensors** promise more natural and comfortable neural input for XR, as users could wear a nearly imperceptible sticker that turns their eye movements and even retinal responses into machine-interpretable signals.



## Headset-Embedded Neuro-Sensors

Another trend is **embedding neural sensors directly into XR headsets and wearables**, creating all-in-one devices that can sense the user’s intent and physiological state. A prime example is OpenBCI’s **Galea** platform, which integrates a suite of EEG, EOG (electro-oculography), EMG, EDA, and PPG sensors into a VR headset strap[[12\]](https://www.roadtovr.com/varjos-aero-open-brain-computer-bci/#:~:text=ImageImage courtesy Varjo%2C OpenBCI). Galea’s first integration with Varjo’s VR headset began beta shipments in 2023[[13\]](https://www.roadtovr.com/varjos-aero-open-brain-computer-bci/#:~:text=,ship date of August 2023)[[14\]](https://www.roadtovr.com/varjos-aero-open-brain-computer-bci/#:~:text=developers in the future more,of serving up dynamic content), targeting researchers and developers. This instrumented headgear can live-capture brain activity and eye signals (alongside heart rate and more) to infer the user’s reactions and intentions in real time. At TED 2023, a Galea-equipped VR setup allowed a paralyzed user to **fly a drone via thought and slight facial muscle movements**, with the system translating his residual eye muscle signals and brain focus into drone controls[[15\]](https://varjo.com/news/assistive-neurotechnology-takes-flight-at-ted2023-with-openbci-and-varjos-virtual-reality#:~:text=What made this drone,that severely limits his mobility)[[16\]](https://varjo.com/news/assistive-neurotechnology-takes-flight-at-ted2023-with-openbci-and-varjos-virtual-reality#:~:text=With OpenBCI’s Galea%2C Bayerlein was,a joystick with their hand). By the late 2020s, we anticipate such sensor-laden headsets moving from labs to specialized applications (training, accessibility) and eventually prosumer XR devices. Tech giants are also investing in this space: **Snap Inc.** acquired neurotech startup NextMind in 2022 to incorporate its non-invasive brain interface into future AR glasses[[17\]](https://techcrunch.com/2022/03/23/snap-buys-mind-controlled-headband-maker-nextmind/#:~:text=Snap this morning confirmed that,”)[[18\]](https://techcrunch.com/2022/03/23/snap-buys-mind-controlled-headband-maker-nextmind/#:~:text=Founded in 2017 by a,go a ways toward solving). NextMind’s headband used an EEG electrode at the visual cortex to detect which object a user was focusing on (via the distinct visual-evoked response), enabling the user to “press” a virtual button just by concentrating on it[[18\]](https://techcrunch.com/2022/03/23/snap-buys-mind-controlled-headband-maker-nextmind/#:~:text=Founded in 2017 by a,go a ways toward solving)[[19\]](https://techcrunch.com/2022/03/23/snap-buys-mind-controlled-headband-maker-nextmind/#:~:text=“This technology monitors neural activity,”). Snap’s vision is that **Spectacles** or other smart glasses could use this brain-sensing to solve AR’s input problem, letting users select or manipulate virtual elements hands-free via intent[[20\]](https://techcrunch.com/2022/03/23/snap-buys-mind-controlled-headband-maker-nextmind/#:~:text=electroencephalogram to detect and read,go a ways toward solving)[[21\]](https://techcrunch.com/2022/03/23/snap-buys-mind-controlled-headband-maker-nextmind/#:~:text=it. Mind,go a ways toward solving). Likewise, startups like **Cognixion** are combining AR displays with EEG: the Cognixion One headset uses scalp electrodes and AI to let locked-in patients spell words in AR by focusing attention on flickering letters[[22\]](https://www.cognixion.com/blog/forbesfeature#:~:text=,intent and attention)[[23\]](https://www.cognixion.com/blog/forbesfeature#:~:text=Our flagship system%2C Cognixion ONE,the future of multimodal neurotechnology). This received FDA Breakthrough Device designation and is entering trials as of 2025. By 2029, **commercial XR headsets with built-in eye and brain sensors** will likely be available – for example, high-end AR glasses that incorporate EOG pads in the nose bridge or EEG contacts in the headband. Even today, we see early signs in products like HP’s **Reverb G2 Omnicept** VR headset, which ships with eye trackers, **pupillometry sensors**, and a heart-rate sensor on board[[24\]](https://cognitive3d.com/blog/hp-cognitive3d/#:~:text=The sensors included are%3A). It uses a machine-learning engine to output real-time “Cognitive Load” metrics from the wearer’s pupil size and physiology[[25\]](https://cognitive3d.com/blog/hp-cognitive3d/#:~:text=,Cognitive Load). This trend of **sensor-fusion headsets** will enable XR systems to adjust to user intent and state – e.g. auto-pausing when cognitive overload is detected, or selecting menu items when the user simply looks and thinks *“click.”*



## Metamaterials and Metasurfaces for Sensors

**Metamaterials** – engineered surfaces with novel electromagnetic or optical properties – are set to boost the performance and miniaturization of eye-based interfaces. In optics, metasurface lenses (metalenses) can drastically shrink eye-tracking and imaging modules. Researchers at Seoul National University created a folded camera lens only 0.7 mm thick using metasurfaces, by routing light through a flat glass substrate with nano-structure reflectors[[26\]](https://www.photonics.com/Articles/Metasurface-Enabled-Camera-Optimized-for-AR-VR/a70527#:~:text=SEOUL%2C South Korea%2C Dec,7 mm)[[27\]](https://www.photonics.com/Articles/Metasurface-Enabled-Camera-Optimized-for-AR-VR/a70527#:~:text=“Our research focuses on efficiently,”). Such an ultrathin lens can be embedded in slim AR/VR glasses to track the user’s gaze or pupil without a protruding camera, making the hardware more seamless. In fact, the meta-optic design achieved a 10° field of view with high resolution in a wafer-thin form, which **“will play an important role in the ... AR industry, where device miniaturization and light weight are essential”**[**[27\]**](https://www.photonics.com/Articles/Metasurface-Enabled-Camera-Optimized-for-AR-VR/a70527#:~:text=“Our research focuses on efficiently,”). Beyond optics, metamaterials are improving bio-signal sensors. One breakthrough is **metamaterial-infused hydrogels** for EEG/ERG electrodes. A 2023 Nature study introduced an “ultrasound metamaterial gel” sensor – a hybrid of biocompatible hydrogel and metamaterial structures – that can wirelessly pick up microvolt neural signals with high fidelity[[28\]](https://www.oreateai.com/blog/crossborder-integration-professor-zang-jianfengs-team-develops-revolutionary-wireless-brain-monitoring-technology/e91912a72db2ccc845bf0284c620472d#:~:text=significant research results from Professor,neuroscience research and clinical applications)[[29\]](https://www.oreateai.com/blog/crossborder-integration-professor-zang-jianfengs-team-develops-revolutionary-wireless-brain-monitoring-technology/e91912a72db2ccc845bf0284c620472d#:~:text=ultrasound). This sensor uses ultrasound both to power the device and transmit brain signals out, achieving a high sensitivity without wires[[30\]](https://www.oreateai.com/blog/crossborder-integration-professor-zang-jianfengs-team-develops-revolutionary-wireless-brain-monitoring-technology/e91912a72db2ccc845bf0284c620472d#:~:text=Compared with traditional brain monitoring,without requiring secondary surgical removal)[[31\]](https://www.oreateai.com/blog/crossborder-integration-professor-zang-jianfengs-team-develops-revolutionary-wireless-brain-monitoring-technology/e91912a72db2ccc845bf0284c620472d#:~:text=reaches microvolt levels%2C enabling it,without requiring secondary surgical removal). While initially developed for implants, the principles could translate to non-invasive setups (e.g. ultrasound-transparent metamaterial patches enhancing EEG detection through hair and skull). Additionally, **metamaterial textiles** (with conductive threads patterned to amplify or guide electromagnetic waves) are being explored for wearable EEG/EMG caps[[32\]](https://www.sciencedirect.com/science/article/abs/pii/B978032391593900002X#:~:text=applications www,power and transfer to)[[33\]](http://www.gao.caltech.edu/uploads/2/6/7/2/26723767/matter-review.pdf#:~:text=,Figure 7D). By 2029, we may see **metasurface-enhanced eye trackers and brain sensors** in XR gear – for instance, meta-lenses enabling compact eye cameras, or metamaterial antenna arrays in headbands that boost the signal-to-noise of neural readings. These advanced materials will help integrate neural interfaces into everyday eyewear by improving signal capture while keeping components ultralight, flat, and almost invisible.



## AI and Real-Time Intent Decoding

Advances in **AI and signal processing** are a crucial enabler for eye-driven XR interfaces, ensuring that raw biosignals can be translated to user intent **quickly and accurately**. Recent years have seen a leap in algorithms for decoding visual evoked potentials (VEPs) and other eye-brain signals. One cutting-edge approach is the use of **code-modulated VEPs (c-VEP)** combined with machine learning. C-VEP BCIs present flickering stimuli encoded with pseudorandom sequences; the brain’s response contains a matching code that can be detected rapidly. This paradigm has achieved information transfer rates higher than any prior EEG-based method[[34\]](https://pmc.ncbi.nlm.nih.gov/articles/PMC11882595/#:~:text=Research on brain,attention to this emerging field)[[35\]](https://pmc.ncbi.nlm.nih.gov/articles/PMC11882595/#:~:text=The c,In). By optimizing stimulus design and employing innovative decoding techniques, researchers have made c-VEP communication both fast and robust – recent systems allow **near real-time selection without lengthy user training**[[36\]](https://pmc.ncbi.nlm.nih.gov/articles/PMC11882595/#:~:text=Complementing the rapid and optimized,Topic%2C  24 Ahmadi et). For example, deep learning models (convolutional and Siamese neural networks) have been shown to decode c-VEP targets in a single trial with ~97% accuracy, vastly outperforming traditional methods[[37\]](https://www.researchgate.net/publication/398134961_Deep_Learning_Architectures_for_Code-Modulated_Visual_Evoked_Potentials_Detection#:~:text=ﬂicker stimulation,models signiﬁcantly outperformed traditional approaches)[[38\]](https://www.researchgate.net/publication/398134961_Deep_Learning_Architectures_for_Code-Modulated_Visual_Evoked_Potentials_Detection#:~:text=the multi,with an average accuracy of). Such AI-powered decoders can discern which visual target a user is focusing on almost instantaneously, which is ideal for XR menus or object selection. Similarly, machine learning is enhancing **pupil-response interfaces**. Researchers found that covert attention to a bright vs. dark flickering target causes a subtle pupil size change, effectively encoding the attended choice. Leveraging this, a 2016 study demonstrated a “mind-writing pupil” interface where a user could type letters by merely **shifting their attention**, with the system decoding the intent from real-time pupillometry signals[[39\]](https://pmc.ncbi.nlm.nih.gov/articles/PMC4743834/#:~:text=We present a new human,computer interfaces to)[[40\]](https://pmc.ncbi.nlm.nih.gov/articles/PMC4743834/#:~:text=selected letter%2C which allows us–with,secure password input). Remarkably, this purely optical method (no electrodes needed) achieved accuracy on par with EEG-based spellers by tracking pupil oscillations to identify the chosen letter in a grid. In practical terms, by 2029 we expect XR platforms to include **on-device AI chips** (like dedicated neural processors in AR glasses) running these sophisticated models to interpret eye signals with millisecond latency. Companies are already moving AI inference to the edge: the HP Omnicept’s built-in processor computes cognitive load from sensor data in real time[[25\]](https://cognitive3d.com/blog/hp-cognitive3d/#:~:text=,Cognitive Load), and Cognixion’s headset uses AI to translate EEG attentional patterns into control commands[[22\]](https://www.cognixion.com/blog/forbesfeature#:~:text=,intent and attention)[[23\]](https://www.cognixion.com/blog/forbesfeature#:~:text=Our flagship system%2C Cognixion ONE,the future of multimodal neurotechnology). Future systems will likely fuse multiple signals – e.g. combining gaze direction, pupil dilation, and VEP/EEG data – in AI models to achieve **high-confidence intent recognition**. This could allow, for example, an AR interface to know a user wants to click an icon when their eyes lock onto it *and* their brain’s visual cortex shows the corresponding response. The convergence of better signal processing (for noise reduction, dynamic calibration) with deep learning classifiers means **truly reactive eye-brain control** of XR is within reach. By late this decade, a user’s intent (to select, navigate, or even express a “no”/“yes”) could be inferred almost as quickly as the thought arises, creating seamless and natural control schemas for XR experiences.



## Prototypes and Roadmaps

Many of these technologies are moving from research to commercialization on clear roadmaps. In the assistive tech arena, **Cognixion’s Axon-R** headset (an FDA-designated device) is slated to bring non-invasive EEG+AR to patients, with Blackrock Neurotech partnering to deploy it to research institutions[[41\]](https://www.cognixion.com/blog/2025/5/12/cxnnews#:~:text=Cognixion and Blackrock Neurotech Expand,interface platform to research institutions). OpenBCI’s Galea, after successful beta trials with Varjo, is expected to broaden availability to developers, potentially spurring an ecosystem of **neuro-responsive VR content** by 2028. Startups like **Neurable** (which demonstrated a VR game controlled by a brain-sensing headband) and major players like **Valve** have indicated they are prototyping brain/vision sensors for next-gen headsets[[42\]](https://www.roadtovr.com/varjos-aero-open-brain-computer-bci/#:~:text=Varjo%2C maker of high,”)[[43\]](https://www.roadtovr.com/varjos-aero-open-brain-computer-bci/#:~:text=OpenBCI initially announced Galea back,interface tech with XR headsets). On the hardware side, display makers (e.g. Meta Materials Inc., Mojo Vision’s display unit) are refining nano-optics and wearable sensors that could be productized in XR devices within a few years. Tech giants are also converging on this space: besides Snap’s bet on NextMind, Meta (Facebook) is exploring neural wristbands and likely keeping an eye on passive BCIs for AR, while Apple’s long-term AR roadmap could integrate health and intent sensors as the tech matures. In summary, the later 2020s will see a **fusion of eye-tracking, neural sensing, and AI** in XR systems. Wearable biosensors (from smart lenses to e-tattoos) will supply rich eye and brain data; metamaterial-based components will make these sensors practically invisible and integrated; and real-time AI will decode the data into actionable commands. These emerging technologies, many already in prototype form with active development toward products, are on track to empower **eye-driven neural interfaces** that feel effortless and instantaneous – turning the user’s gaze and mental focus into the ultimate AR/VR controller[[20\]](https://techcrunch.com/2022/03/23/snap-buys-mind-controlled-headband-maker-nextmind/#:~:text=electroencephalogram to detect and read,go a ways toward solving)[[21\]](https://techcrunch.com/2022/03/23/snap-buys-mind-controlled-headband-maker-nextmind/#:~:text=it. Mind,go a ways toward solving). The result will be XR experiences that respond to what you *intend* to do, not just what you explicitly touch or say, marking a significant step toward more natural human-computer interactions.

**Sources:** The information above is supported by recent developments and reports, including academic research on wearable EEG/EOG tattoos[[6\]](https://spectrum.ieee.org/electronic-tattoo#:~:text=Researchers at the University of,movements to gauge mental strain)[[7\]](https://www.ae.utexas.edu/news/stressed-or-bored-at-work-new-electronic-tattoo-can-help#:~:text=The e,for comfort and clear signals), industry news on AR contact lens prototypes[[3\]](https://www.xpanceo.com/newsroom/gitex-press-release#:~:text=Initial testing of a fully,integrated into a single device)[[4\]](https://www.xpanceo.com/newsroom/gitex-press-release#:~:text=Another innovation presented was the,jpg), product announcements for sensor-integrated headsets[[12\]](https://www.roadtovr.com/varjos-aero-open-brain-computer-bci/#:~:text=ImageImage courtesy Varjo%2C OpenBCI)[[24\]](https://cognitive3d.com/blog/hp-cognitive3d/#:~:text=The sensors included are%3A), and demonstrations of AI-driven BCI performance gains[[34\]](https://pmc.ncbi.nlm.nih.gov/articles/PMC11882595/#:~:text=Research on brain,attention to this emerging field)[[38\]](https://www.researchgate.net/publication/398134961_Deep_Learning_Architectures_for_Code-Modulated_Visual_Evoked_Potentials_Detection#:~:text=the multi,with an average accuracy of). These connected sources illustrate the rapid progress and convergence in biosensing hardware and AI needed for next-generation eye-controlled XR interfaces.





[[1\]](https://www.uctoday.com/immersive-workplace-xr-tech/augmented-reality-contact-lenses-will-they-ever-be-a-reality/#:~:text=company that was previously leading,connected to an external battery) [[2\]](https://www.uctoday.com/immersive-workplace-xr-tech/augmented-reality-contact-lenses-will-they-ever-be-a-reality/#:~:text=alongside software application prototyping%2C were,still necessary) [[5\]](https://www.uctoday.com/immersive-workplace-xr-tech/augmented-reality-contact-lenses-will-they-ever-be-a-reality/#:~:text=Innovators believe high,to treat visual health issues) Augmented Reality Contact Lenses: Will They Ever Be a Reality? - UC Today

https://www.uctoday.com/immersive-workplace-xr-tech/augmented-reality-contact-lenses-will-they-ever-be-a-reality/

[[3\]](https://www.xpanceo.com/newsroom/gitex-press-release#:~:text=Initial testing of a fully,integrated into a single device) [[4\]](https://www.xpanceo.com/newsroom/gitex-press-release#:~:text=Another innovation presented was the,jpg) XPANCEO

https://www.xpanceo.com/newsroom/gitex-press-release

[[6\]](https://spectrum.ieee.org/electronic-tattoo#:~:text=Researchers at the University of,movements to gauge mental strain) [[11\]](https://spectrum.ieee.org/electronic-tattoo#:~:text=circuit toward the temples%2C cheeks%2C,and to stabilize the signal) An Electronic Tattoo for the Face Monitors Mental Strain - IEEE Spectrum

https://spectrum.ieee.org/electronic-tattoo

[[7\]](https://www.ae.utexas.edu/news/stressed-or-bored-at-work-new-electronic-tattoo-can-help#:~:text=The e,for comfort and clear signals) [[8\]](https://www.ae.utexas.edu/news/stressed-or-bored-at-work-new-electronic-tattoo-can-help#:~:text=from the brain and eye,for comfort and clear signals) [[9\]](https://www.ae.utexas.edu/news/stressed-or-bored-at-work-new-electronic-tattoo-can-help#:~:text=The device didn’t stop at,can potentially predict mental fatigue) [[10\]](https://www.ae.utexas.edu/news/stressed-or-bored-at-work-new-electronic-tattoo-can-help#:~:text=researchers trained a computer model,can potentially predict mental fatigue) Stressed or Bored at Work? New Electronic Tattoo Can Help

https://www.ae.utexas.edu/news/stressed-or-bored-at-work-new-electronic-tattoo-can-help

[[12\]](https://www.roadtovr.com/varjos-aero-open-brain-computer-bci/#:~:text=ImageImage courtesy Varjo%2C OpenBCI) [[13\]](https://www.roadtovr.com/varjos-aero-open-brain-computer-bci/#:~:text=,ship date of August 2023) [[14\]](https://www.roadtovr.com/varjos-aero-open-brain-computer-bci/#:~:text=developers in the future more,of serving up dynamic content) [[42\]](https://www.roadtovr.com/varjos-aero-open-brain-computer-bci/#:~:text=Varjo%2C maker of high,”) [[43\]](https://www.roadtovr.com/varjos-aero-open-brain-computer-bci/#:~:text=OpenBCI initially announced Galea back,interface tech with XR headsets) Varjo's Enthusiast Grade VR Headset is Getting a Brain-computer Interface (and it's not cheap)

https://www.roadtovr.com/varjos-aero-open-brain-computer-bci/

[[15\]](https://varjo.com/news/assistive-neurotechnology-takes-flight-at-ted2023-with-openbci-and-varjos-virtual-reality#:~:text=What made this drone,that severely limits his mobility) [[16\]](https://varjo.com/news/assistive-neurotechnology-takes-flight-at-ted2023-with-openbci-and-varjos-virtual-reality#:~:text=With OpenBCI’s Galea%2C Bayerlein was,a joystick with their hand) Assistive Neurotechnology Takes Flight at TED2023

https://varjo.com/news/assistive-neurotechnology-takes-flight-at-ted2023-with-openbci-and-varjos-virtual-reality

[[17\]](https://techcrunch.com/2022/03/23/snap-buys-mind-controlled-headband-maker-nextmind/#:~:text=Snap this morning confirmed that,”) [[18\]](https://techcrunch.com/2022/03/23/snap-buys-mind-controlled-headband-maker-nextmind/#:~:text=Founded in 2017 by a,go a ways toward solving) [[19\]](https://techcrunch.com/2022/03/23/snap-buys-mind-controlled-headband-maker-nextmind/#:~:text=“This technology monitors neural activity,”) [[20\]](https://techcrunch.com/2022/03/23/snap-buys-mind-controlled-headband-maker-nextmind/#:~:text=electroencephalogram to detect and read,go a ways toward solving) [[21\]](https://techcrunch.com/2022/03/23/snap-buys-mind-controlled-headband-maker-nextmind/#:~:text=it. Mind,go a ways toward solving) Snap buys mind-controlled headband maker NextMind

https://techcrunch.com/2022/03/23/snap-buys-mind-controlled-headband-maker-nextmind/

[[22\]](https://www.cognixion.com/blog/forbesfeature#:~:text=,intent and attention) [[23\]](https://www.cognixion.com/blog/forbesfeature#:~:text=Our flagship system%2C Cognixion ONE,the future of multimodal neurotechnology) Cognixion Featured in Forbes: Expanding Access to BCI Without Surgery — Cognixion®

https://www.cognixion.com/blog/forbesfeature

[[24\]](https://cognitive3d.com/blog/hp-cognitive3d/#:~:text=The sensors included are%3A) [[25\]](https://cognitive3d.com/blog/hp-cognitive3d/#:~:text=,Cognitive Load) Cognitive3D Launches Support for HP Reverb G2 Omnicept Edition: The First Native Biometric Capable VR HMD | Cognitive3D

https://cognitive3d.com/blog/hp-cognitive3d/

[[26\]](https://www.photonics.com/Articles/Metasurface-Enabled-Camera-Optimized-for-AR-VR/a70527#:~:text=SEOUL%2C South Korea%2C Dec,7 mm) [[27\]](https://www.photonics.com/Articles/Metasurface-Enabled-Camera-Optimized-for-AR-VR/a70527#:~:text=“Our research focuses on efficiently,”) Metasurface-Enabled Camera Optimized for AR/VR Devices | Research & Technology | Nov 2024 | Photonics Spectra 

https://www.photonics.com/Articles/Metasurface-Enabled-Camera-Optimized-for-AR-VR/a70527

[[28\]](https://www.oreateai.com/blog/crossborder-integration-professor-zang-jianfengs-team-develops-revolutionary-wireless-brain-monitoring-technology/e91912a72db2ccc845bf0284c620472d#:~:text=significant research results from Professor,neuroscience research and clinical applications) [[29\]](https://www.oreateai.com/blog/crossborder-integration-professor-zang-jianfengs-team-develops-revolutionary-wireless-brain-monitoring-technology/e91912a72db2ccc845bf0284c620472d#:~:text=ultrasound) [[30\]](https://www.oreateai.com/blog/crossborder-integration-professor-zang-jianfengs-team-develops-revolutionary-wireless-brain-monitoring-technology/e91912a72db2ccc845bf0284c620472d#:~:text=Compared with traditional brain monitoring,without requiring secondary surgical removal) [[31\]](https://www.oreateai.com/blog/crossborder-integration-professor-zang-jianfengs-team-develops-revolutionary-wireless-brain-monitoring-technology/e91912a72db2ccc845bf0284c620472d#:~:text=reaches microvolt levels%2C enabling it,without requiring secondary surgical removal) Cross-Border Integration: Professor Zang Jianfeng's Team Develops Revolutionary Wireless Brain Monitoring Technology - Oreate AI Blog

https://www.oreateai.com/blog/crossborder-integration-professor-zang-jianfengs-team-develops-revolutionary-wireless-brain-monitoring-technology/e91912a72db2ccc845bf0284c620472d

[[32\]](https://www.sciencedirect.com/science/article/abs/pii/B978032391593900002X#:~:text=applications www,power and transfer to) Progress in physiological textile sensors for biomedical applications

https://www.sciencedirect.com/science/article/abs/pii/B978032391593900002X

[[33\]](http://www.gao.caltech.edu/uploads/2/6/7/2/26723767/matter-review.pdf#:~:text=,Figure 7D) [PDF] Skin-Interfaced Sensors in Digital Medicine: from Materials to ...

http://www.gao.caltech.edu/uploads/2/6/7/2/26723767/matter-review.pdf

[[34\]](https://pmc.ncbi.nlm.nih.gov/articles/PMC11882595/#:~:text=Research on brain,attention to this emerging field) [[35\]](https://pmc.ncbi.nlm.nih.gov/articles/PMC11882595/#:~:text=The c,In) [[36\]](https://pmc.ncbi.nlm.nih.gov/articles/PMC11882595/#:~:text=Complementing the rapid and optimized,Topic%2C  24 Ahmadi et) Editorial: The role of code-modulated evoked potentials in next-generation brain-computer interfacing - PMC 

https://pmc.ncbi.nlm.nih.gov/articles/PMC11882595/

[[37\]](https://www.researchgate.net/publication/398134961_Deep_Learning_Architectures_for_Code-Modulated_Visual_Evoked_Potentials_Detection#:~:text=ﬂicker stimulation,models signiﬁcantly outperformed traditional approaches) [[38\]](https://www.researchgate.net/publication/398134961_Deep_Learning_Architectures_for_Code-Modulated_Visual_Evoked_Potentials_Detection#:~:text=the multi,with an average accuracy of) (PDF) Deep Learning Architectures for Code-Modulated Visual Evoked Potentials Detection

https://www.researchgate.net/publication/398134961_Deep_Learning_Architectures_for_Code-Modulated_Visual_Evoked_Potentials_Detection

[[39\]](https://pmc.ncbi.nlm.nih.gov/articles/PMC4743834/#:~:text=We present a new human,computer interfaces to) [[40\]](https://pmc.ncbi.nlm.nih.gov/articles/PMC4743834/#:~:text=selected letter%2C which allows us–with,secure password input) The Mind-Writing Pupil: A Human-Computer Interface Based on Decoding of Covert Attention through Pupillometry - PMC 

https://pmc.ncbi.nlm.nih.gov/articles/PMC4743834/

[[41\]](https://www.cognixion.com/blog/2025/5/12/cxnnews#:~:text=Cognixion and Blackrock Neurotech Expand,interface platform to research institutions) Cognixion and Blackrock Neurotech Expand Access to Non-Invasive ...

https://www.cognixion.com/blog/2025/5/12/cxnnews